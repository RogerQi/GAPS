name: 'voc2012_semantic_segmentation_deeplabv3_resnet101'
task: 'semantic_segmentation'
input_dim: (3, 513, 513) # Input dim follows https://github.com/chenxi116/DeepLabv3.pytorch
num_classes: 21 # (20 annotated class + 1 background)
save_model: True

SYSTEM:
  num_workers: 16

BACKBONE:
  network: 'deeplabv3_resnet101'
  pooling: False
  use_pretrained: True
  pretrained_path: "./pretrained_model/resnet101-5d3b4d8f.pth"

CLASSIFIER:
  classifier: "plain_c1"

LOSS:
  loss: 'semantic_nllloss'

DATASET:
  dataset: 'VOC2012_seg'
  cache_all_data: True
  TRANSFORM:
    TRAIN:
      transforms: ('normalize', )
      joint_transforms: ('joint_random_resized_crop', 'joint_random_horizontal_flip')
      TRANSFORMS_DETAILS:
        NORMALIZE:
          mean: (0.4700, 0.4468, 0.4076)
          sd: (0.2439, 0.2390, 0.2420)
        crop_size: (513, 513)
    TEST:
      transforms: ('normalize', )
      TRANSFORMS_DETAILS:
        NORMALIZE:
          mean: (0.4700, 0.4468, 0.4076)
          sd: (0.2439, 0.2390, 0.2420)
        crop_size: (513, 513)

TRAIN:
  max_epochs: 50
  batch_size: 16
  initial_lr: 0.007
  lr_scheduler: "polynomial"
  log_interval: 20
  OPTIMIZER:
    type: "SGD"
    momentum: 0.9
    weight_decay: 0.0001


TEST:
  batch_size: 1
